# Google ads for shared ChatGPT, Grok guides push macOS infostealer malware

![Google ads for shared ChatGPT, Grok guides push macOS infostealer malware](https://www.bleepstatic.com/content/hl-images/2023/09/11/apple_triangle.jpg)

A new AMOS infostealer campaign is abusing Google search ads to lure users into Grok and ChatGPT conversations that appear to offer “helpful” instructions but ultimately lead to installing the AMOS info-stealing malware on macOS.

The campaign was [first spotted](http://www.kaspersky.co.uk/blog/share-chatgpt-chat-clickfix-macos-amos-infostealer/29796/) by researchers at cybersecurity company Kaspersky yesterday, while Huntress managed security platform published a more detailed report earlier today.

The [ClickFix](https://www.bleepingcomputer.com/tag/clickfix/) attack begins with victims searching for macOS-related terms, such as maintenance questions, problem-solving, or for Atlas - OpenAI's AI-powered web browser for macOS.

Google advertisement link directly to ChatGPT and Grok conversations that had been publicly shared in preparation for the attack. The chats are hosted on the legitimate LLM platforms and contain the malicious instructions used to install the malware.

![Malicious ChatGPT (left) and Grok (right) conversations](https://www.bleepstatic.com/images/news/u/1220909/2025/December/chatgpt.jpg)

**Malicious ChatGPT (left) and Grok (right) conversations**  
_Source: Huntress_

"During our investigation, the Huntress team reproduced these poisoned results across multiple variations of the same question, 'how to clear data on iMac,' 'clear system data on iMac,' 'free up storage on Mac,' confirming this isn't an isolated result but a deliberate, widespread poisoning campaign targeting common troubleshooting queries," [Huntress researchers explain](https://www.huntress.com/blog/amos-stealer-chatgpt-grok-ai-trust).

If users fall for the trick and execute the commands from the AI chat in macOS Terminal, a base64-encoded URL decodes into a bash script (update) that loads a fake password prompt dialog.

![The bash script](https://www.bleepstatic.com/images/news/u/1220909/2025/December/bashscript.jpg)

**The bash script**  
_Source: Huntress_

When the password is provided, the script validates, stores, and uses it to execute privileged commands, such as downloading the AMOS infostealer and executing the malware with root-level privileges.

AMOS was [first documented in April 2023](https://www.bleepingcomputer.com/news/security/new-atomic-macos-info-stealing-malware-targets-50-crypto-wallets/). It is a malware-as-a-service (MaaS) operation that rents the infostealer $1,000/month, targeting macOS systems exclusively.

Earlier this year, AMOS [added a backdoor module](https://www.bleepingcomputer.com/news/security/atomic-macos-infostealer-adds-backdoor-for-persistent-attacks/) that lets operators execute commands on infected hosts, log key strokes, and drop additional payloads.

AMOS is dropped on _/Users/$USER/_ as a hidden file (.helper). When launched, it scans the applications folder for Ledger Wallet and Trezor Suite. If found, it overwrites them with trojanized versions that prompt the victim to enter their seed phrase "for security" reasons.

**Replacing crypto wallet apps with trojanized versions**  
_Source: Huntress_

AMOS also targets cryptocurrency wallets from Electrum, Exodus, MetaMask, Ledger Live, Coinbase Wallet, and others; browser data such as cookies, saved passwords, autofill data, and session tokens; macOS Keychain data such as app passwords and Wi-Fi credentials; and files on the filesystem.

Persistence is achieved via a LaunchDaemon (com.finder.helper.plist) running a hidden AppleScript which acts as a watchdog loop, restarting the malware within one second if terminated.

These latest ClickFix attacks are yet another example of threat actors experimenting with new ways to exploit legitimate, popular platforms like OpenAI and X.

Users need to be vigilant and avoid executing commands they found online, especially if they don't fully understand what they do.

Kaspersky noted that, even after reaching these manipulated LLM conversations, a simple follow-up question asking ChatGPT if the provided instructions are safe to execute reveals that they aren't.